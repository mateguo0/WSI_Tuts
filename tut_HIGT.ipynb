{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIGT Usage Tutorial:\n",
    "\n",
    "HIGT uses thumbnail, 5x and 10x three levels of whole slide microscope image (WSI) data to form the input data. In order to allow readers to better understand and practice, we provide a complete tutorial. You can download the [TCGA data](https://portal.gdc.cancer.gov/) through the link below. This tutorial is mainly built on [CLAM](https://github.com/mahmoodlab/CLAM), but we made some changes to suit specific needs. \n",
    "\n",
    "This tutorial is divided into three main parts: Preprocessing, GPU Training, Testing and Evaluation. The details are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing:\n",
    "\n",
    "This section is mainly divided into five parts: Basic Information Statistics, Patch Segmentation, Feature Extraction, Hierarchical Graph (Tree) Generation and Dataset Split. The details of each part are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic Information Statistics:\n",
    "\n",
    "Use the `generate_pl_bm` function to analyze the basic information of Whole Slide Images (WSI). It mainly realizes the following two functions:\n",
    "\n",
    "1. **Objective Lens Magnification**:   \n",
    "Count and record the default objective lens magnification information for the target WSI file. The recorded data is saved in a file named `bm.csv`.\n",
    "\n",
    "2. **Process List Generation**:   \n",
    "Based on the target objective lens magnification and the cutting block size of the target objective lens magnification, generate the corresponding `pl_mag{target_magnification}x_patch{base_patch_size}_{target_patch_size}.csv`.\n",
    "\n",
    "3. **Data cleaning**:  \n",
    "Cleared some WSIs without default objective magnifications.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--WSI_dir`**: Saving directory of the WSI file.\n",
    "- **`--csv_dir`**: Saving directory of the CSV file.\n",
    "- **`--base_patch_size`**: The size of the cutting block used by default at the target objective lens magnification.\n",
    "- **`--target_mag`**: Target objective lens magnification.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "In the following experiments, we demonstrate with a data set with a default objective lens magnification of 20 magnifications, called `WSI_bm20`. The codes for basic statistics can be structured as follows:\n",
    "\n",
    "```python\n",
    "generate_pl_bm(\n",
    "        WSI_dir=\"/path/to/exp/WSI_bm20\", \n",
    "        save_dir=\"/path/to/dataset_csv/WSI_bm20/\", \n",
    "        base_patch_size=512, \n",
    "        target_mag=5\n",
    ")\n",
    "\n",
    "generate_pl_bm(\n",
    "        WSI_dir=\"/path/to/exp/WSI_bm20\", \n",
    "        save_dir=\"/path/to/dataset_csv/WSI_bm20/\", \n",
    "        base_patch_size=512, \n",
    "        target_mag=10\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your specific setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pl_bm(\n",
    "        WSI_dir, \n",
    "        save_dir, \n",
    "        base_patch_size, \n",
    "        target_mag\n",
    "    ):\n",
    "    import openslide, glob\n",
    "    import pandas as pd\n",
    "    '''\n",
    "    WSI_path: path for WSI files\n",
    "        WSI_path/1.svs,\n",
    "        WSI_path/2.svs,\n",
    "        WSI_path/3.svs,\n",
    "        ...\n",
    "    save_path: path for process list csv file and base magnification csv file\n",
    "        save_path/pl_mag20x_patch512_1024.csv,\n",
    "        save_path/pl_mag20x_patch512_2048.csv,\n",
    "        ...\n",
    "        save_path/bm_mag20x_patch512.csv\n",
    "    '''\n",
    "    process_list = {} \n",
    "    base_mag_csv = {\n",
    "        \"slide_path\": [],\n",
    "        \"base_mag\": []\n",
    "    }\n",
    "\n",
    "    for WSI in glob.glob(WSI_dir+\"/*\"):\n",
    "        slide = openslide.open_slide(WSI)\n",
    "        wsi_name = WSI.split(\"/\")[-1]\n",
    "        if slide.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER) == None:\n",
    "            continue\n",
    "\n",
    "        base_mag = int(slide.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER))\n",
    "        target_min_patch_size = int(base_patch_size*(base_mag/target_mag))\n",
    "\n",
    "        # Update for process_list\n",
    "        if target_min_patch_size not in process_list:\n",
    "            process_list[target_min_patch_size] = [wsi_name]\n",
    "        else:\n",
    "            process_list[target_min_patch_size].append(wsi_name)\n",
    "\n",
    "        # Update for base_mag_csv\n",
    "        base_mag_csv[\"slide_path\"].append(WSI)\n",
    "        base_mag_csv[\"base_mag\"].append(base_mag)\n",
    "\n",
    "        # save base_mag_csv.csv\n",
    "        df = pd.DataFrame(base_mag_csv)\n",
    "        df.to_csv(save_dir+f\"bm_mag{target_mag}x_patch{base_patch_size}.csv\")\n",
    "\n",
    "        # save patch_size_i process_list.csv\n",
    "        for k in process_list.keys():\n",
    "            df = pd.DataFrame({\n",
    "                \"slide_id\": process_list[k]\n",
    "            })\n",
    "            df.to_csv(save_dir+f\"pl_mag{target_mag}x_patch{base_patch_size}_{k}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Patch Segmentation:\n",
    "\n",
    "Patch segmentation is required to be performed based on the default objective magnification of the slide, in conjunction with the cutting block size under the target magnification. This process results in the acquisition of the cutting block size under the default objective magnification. \n",
    "\n",
    "For HIGT, the cutting results at 5x and 10x magnification are required. First, the patch cutting at 5x magnification is performed based on CLAMâ€™s create_patches_fp, and then the cutting results at 10x magnification of the corresponding data set are generated according to the cutting results at 5x magnification and the `generate_coords_file` function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Patch segmentation at 5x magnification:\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--source`**: Slide file root directory.\n",
    "- **`--save_dir`**: Directory to save the results of patch cutting.\n",
    "- **`--patch_size`**: The size of the patch at the default objective magnification.\n",
    "- **`--step_size`**: Step size for cutting patches. If no overlap is required, this should be the same as `patch_size`.\n",
    "- **`--seg`**: Flag to indicate whether to generate a mask.\n",
    "- **`--patch`**: Flag to indicate whether to generate a patch.\n",
    "- **`--stitch`**: Flag to indicate whether to generate a stitch.\n",
    "- **`--process_list`**: Process list, mainly utilizing the `slide_id` column; other columns can remain at their default settings.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "The command line for patch segmentation can be structured as follows:\n",
    "\n",
    "```bash\n",
    "python create_patches_fp.py --source /path/to/exp/WSI_bm20 --save_dir /path/to/exp/segmentation/WSI_bm20/mag5x_patch512_2048 --patch_size 2048 --step_size 2048 --seg --patch --stitch --process_list /path/to/dataset_csv/WSI_bm20/pl_mag5x_patch512_2048.csv\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your specific setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Generation of patch segmentation at 10x magnification:\n",
    "\n",
    "Generate the segmentation results at 10x magnification according to the cutting results at 5x magnification and the `generate_coords_file` function below.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--ori_mag_seg_path`**: Saving path of the generated target magnification segmentation result.\n",
    "- **`--target_mag`**: Target magnification.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The code for the generation of the patch segmentation can be structured as follows:\n",
    "\n",
    "```python\n",
    "generate_coords_file(\n",
    "    ori_mag_seg_path=\"/path/to/exp/segmentation/WSI_bm20/mag5x_patch512_2048\", \n",
    "    target_mag=10\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your specific setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coords_file(ori_mag_seg_path, target_mag):\n",
    "    \n",
    "    import os, h5py, glob\n",
    "    import numpy as np\n",
    "    from wsi_core.wsi_utils import save_hdf5\n",
    "\n",
    "    cur_mag = int(ori_mag_seg_path.split(\"/\")[-1].split(\"_\")[0].replace(\"mag\",\"\").replace(\"x\",\"\"))\n",
    "    cur_patch_size = int(ori_mag_seg_path.split(\"/\")[-1].split(\"_\")[-1])\n",
    "    target_patch_size = int(cur_patch_size/int(target_mag/cur_mag))\n",
    "\n",
    "    for h5 in glob.glob(ori_mag_seg_path+\"/patches/*\"):\n",
    "        h5_content = h5py.File(h5,'r')\n",
    "\n",
    "        coords = h5_content[\"coords\"][:]\n",
    "        save_path = \"/\".join(ori_mag_seg_path.split(\"/\")[:-1])+f\"/mag{target_mag}x_patch512_{target_patch_size}/patches/\"\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        attr = {\n",
    "            'patch_size' :            target_patch_size, \n",
    "            'patch_level' :           h5_content[\"coords\"].attrs[\"patch_level\"],\n",
    "            'downsample':             h5_content[\"coords\"].attrs[\"downsample\"],\n",
    "            'downsampled_level_dim' : h5_content[\"coords\"].attrs[\"downsampled_level_dim\"],\n",
    "            'level_dim':              h5_content[\"coords\"].attrs[\"level_dim\"],\n",
    "            'name':                   h5_content[\"coords\"].attrs[\"name\"],\n",
    "            'save_path':              save_path\n",
    "        }\n",
    "\n",
    "        h5_content.close()\n",
    "        coords_ = []\n",
    "        for coord in coords:\n",
    "            x,y = coord\n",
    "            coords_.append([x,y])\n",
    "            coords_.append([x+target_patch_size,y])\n",
    "            coords_.append([x,y+target_patch_size])\n",
    "            coords_.append([x+target_patch_size,y+target_patch_size])\n",
    "            \n",
    "        coords_ = np.array(coords_).astype(coords.dtype)\n",
    "        unique_coords = np.unique(coords_, axis=0)\n",
    "\n",
    "        save_hdf5(\n",
    "            save_path+h5.split(\"/\")[-1], \n",
    "            {\"coords\": unique_coords}, \n",
    "            {\"coords\":attr}, \n",
    "            mode=\"w\"\n",
    "        )\n",
    "\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature Extraction:\n",
    "\n",
    "Feature extraction is performed based on the results of patch segmentation. This section describes the parameters and provides examples for performing feature extraction. The feature extraction of the thumbnail part can be implemented using the following `extract_features_thumb` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Feature Extraction at 5x and 10x magnification:\n",
    "\n",
    "Feature extraction of patch images based on existing patch cutting results.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--csv_path`**: Specifies the path to the process list. It mainly uses the `slide_id` column, and other columns can be omitted.\n",
    "- **`--data_h5_dir`**: Specifies the root directory for the results of patch segmentation.\n",
    "- **`--data_slide_dir`**: Specifies the root directory for slide files.\n",
    "- **`--feat_dir`**: Specifies the root directory for saving extracted features.\n",
    "- **`--batch_size`**: Defines the batch size for processing (e.g., `32`, `64`, etc.).\n",
    "- **`--target_patch_size`**: Specifies the size of the input image.\n",
    "- **`--slide_ext`**: Defines the file suffix for slide files.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The command line for feature extraction can be structured as follows:\n",
    "\n",
    "```bash\n",
    "# 5x Magnification\n",
    "python feature_extraction.py --csv_path /path/to/dataset_csv/WSI_bm20/pl_mag5x_patch512_2048.csv --data_h5_dir /path/to/exp/segmentation/WSI_bm20/mag5x_patch512_2048 --data_slide_dir /path/to/exp/WSI_bm20 --feat_dir /path/to/exp/extracted_feature/WSI_bm20/mag5x_patch512_2048 --batch_size 256 --target_patch_size 256 --slide_ext .svs\n",
    "\n",
    "# 10x Magnification\n",
    "python feature_extraction.py --csv_path /path/to/dataset_csv/WSI_bm20/pl_mag10x_patch512_1024.csv --data_h5_dir /path/to/exp/segmentation/WSI_bm20/mag10x_patch512_1024 --data_slide_dir /path/to/exp/WSI_bm20 --feat_dir /path/to/exp/extracted_feature/WSI_bm20/mag10x_patch512_1024 --batch_size 256 --target_patch_size 256 --slide_ext .svs\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your specific setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Feature Extraction for Thumbnail:\n",
    "\n",
    "Based on the `extract_features_thumb` function, extract features from WSI thumbnail images.\n",
    "\n",
    "##### (1)Parameter Description:\n",
    "- **`--data_slide_dir`**: Specifies the root directory for slide files.\n",
    "- **`--cuda_id`**: ID of GPUs to use.\n",
    "- **`--feat_dir`**: Specifies the root directory for saving extracted features.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The code for feature extraction of thumbnail can be structured as follows:\n",
    "\n",
    "```python\n",
    "feature_extract_thumb(\n",
    "    data_slide_dir=\"/path/to/exp/WSI_bm20\",\n",
    "    cuda_id=0,\n",
    "    feat_dir=\"/path/to/exp/extracted_feature/WSI_bm20/mag10x_patch512_1024\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your specific setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract_thumb(\n",
    "    data_slide_dir,\n",
    "    cuda_id,\n",
    "    feat_dir\n",
    "):\n",
    "    import openslide, glob, torch, h5py, os\n",
    "    from torchvision import transforms\n",
    "    from models.resnet_custom import resnet50_baseline\n",
    "    device = torch.device(f\"cuda:{cuda_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = resnet50_baseline(pretrained=True).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for slide_path in glob.glob(data_slide_dir+\"/*\"):\n",
    "            slide = openslide.OpenSlide(slide_path)\n",
    "            thumbnail_image = slide.get_thumbnail((512,512))\n",
    "            thumbnail_tensor = transform(thumbnail_image).to(device)\n",
    "\n",
    "            thumbnail_feat = model(thumbnail_tensor.unsqueeze(0))\n",
    "            out_root_path = f'{feat_dir}/thumbnail/'\n",
    "            if not os.path.exists(out_root_path):\n",
    "                os.makedirs(out_root_path)\n",
    "            with h5py.File(out_root_path+slide_path.split(\"/\")[-1]+\".h5\", 'w') as h5f:\n",
    "                h5f.create_dataset('features', data=thumbnail_feat.cpu().numpy())\n",
    "                h5f.create_dataset('coords', data=[[0,0]])\n",
    "            print(slide_path.split(\"/\")[-1],\" done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph (Tree) Generation:\n",
    "Based on the features of thumbnail, 5x and 10x extracted above, use the `generate_graph_tree()` function to construct a heterogeneous graph (tree),\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--target_mags`**: List of all target mags.\n",
    "- **`--bm_path`**: Saving path of base magnification.\n",
    "- **`--feature_path`**: Specifies the root directory for saving extracted features.\n",
    "- **`--base_patch_size`**: The patch size at the target magnification.\n",
    "- **`--save_path`**: Save path of the generated graphs.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The code for graph generation can be structured as follows:\n",
    "\n",
    "```python\n",
    "generate_graph_tree(\n",
    "     target_mags = [float(\"inf\"), 5, 10],\n",
    "     bm_path = \"/path/to/dataset_csv/WSI_bm20/bm.csv\",\n",
    "     feature_path = \"/path/to/exp/extracted_feature/WSI_bm20/\",\n",
    "     base_patch_size = 512,\n",
    "     save_path = \"/path/to/exp/extracted_feature/WSI_bm20/\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_tree(\n",
    "    target_mags, bm_path, feature_path, base_patch_size, save_path\n",
    "):\n",
    "    import os, h5py, torch\n",
    "    from torch_geometric.data import Data\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    def get_edge_index(feature_i, region_patch_size, patch_patch_size):\n",
    "    \n",
    "        start = []\n",
    "        end = []\n",
    "\n",
    "        len_thumbnail = 1\n",
    "    \n",
    "        region_mag = list(feature_i.keys())[1]\n",
    "        region_features = feature_i[region_mag][\"features\"]\n",
    "        region_coords = feature_i[region_mag][\"coords\"]\n",
    "        len_region_node = len(region_features)\n",
    "\n",
    "        patch_mag = list(feature_i.keys())[2]\n",
    "        patch_features = feature_i[patch_mag][\"features\"]\n",
    "        patch_coords = feature_i[patch_mag][\"coords\"]\n",
    "        len_patch_node = len(patch_features)\n",
    "        \n",
    "        # 1. region_level\n",
    "        for i in range(len_region_node):\n",
    "\n",
    "            # 1. thumbnail <=> region\n",
    "            region_index = i+len_thumbnail\n",
    "            x,y = region_coords[i]\n",
    "            start.append(0)\n",
    "            end.append(region_index)\n",
    "            start.append(region_index)\n",
    "            end.append(0)\n",
    "\n",
    "            # 2. region => region\n",
    "            nb_coord_list = [\n",
    "                [x-region_patch_size, y-region_patch_size], [x, y-region_patch_size], [x+region_patch_size, y+region_patch_size],\n",
    "                [x-region_patch_size, y], [x+region_patch_size, y],\n",
    "                [x-region_patch_size, y+region_patch_size], [x, y+region_patch_size], [x+region_patch_size, y+region_patch_size]\n",
    "            ]\n",
    "            for xy_ in nb_coord_list:\n",
    "                x_, y_ = xy_\n",
    "                if np.any(np.all(region_coords == [x_, y_], axis=1)):\n",
    "                    to_region_index = np.where((region_coords==(x_, y_)).all(axis=1))[0][0]+len_thumbnail\n",
    "                    start.append(region_index)\n",
    "                    end.append(to_region_index)\n",
    "                    start.append(to_region_index)\n",
    "                    end.append(region_index)\n",
    "\n",
    "        # 2. patch_level\n",
    "        for i in range(len_patch_node):\n",
    "            patch_index = i+len_thumbnail+len_region_node\n",
    "            x,y = patch_coords[i]\n",
    "\n",
    "            # 1. patch <=> region\n",
    "            region_coord = (patch_coords[i]//2048)*2048\n",
    "            x_region, y_region = region_coord\n",
    "            region_index = np.where((region_coords==(x_region, y_region)).all(axis=1))[0][0]+len_thumbnail\n",
    "            start.append(region_index)\n",
    "            end.append(patch_index)\n",
    "            start.append(patch_index)\n",
    "            end.append(region_index)\n",
    "\n",
    "            # 2. patch <=> patch\n",
    "            nb_coord_list = [\n",
    "                [x-patch_patch_size, y-patch_patch_size], [x, y-patch_patch_size], [x+patch_patch_size, y+patch_patch_size],\n",
    "                [x-patch_patch_size, y], [x+patch_patch_size, y],\n",
    "                [x-patch_patch_size, y+patch_patch_size], [x, y+patch_patch_size], [x+patch_patch_size, y+patch_patch_size]\n",
    "            ]\n",
    "            for xy_ in nb_coord_list:\n",
    "                x_,y_ = xy_\n",
    "                if np.any(np.all(patch_coords == [x_, y_], axis=1)):\n",
    "                    to_patch_index = np.where((patch_coords==(x_,y_)).all(axis=1))[0][0]+len_thumbnail+len_region_node\n",
    "                    start.append(patch_index)\n",
    "                    end.append(to_patch_index)\n",
    "                    start.append(to_patch_index)\n",
    "                    end.append(patch_index)\n",
    "        return [start, end]\n",
    "\n",
    "    all_data = {}\n",
    "    bm = pd.read_csv(bm_path)\n",
    "    \n",
    "    for slide_path,base_mag in zip(bm['slide_path'],bm['base_mag']):\n",
    "        wsi_name = slide_path.split(\"/\")[-1].replace(\".svs\",\"\")\n",
    "        h5_name = wsi_name+\".h5\"\n",
    "        \n",
    "        feature_i = {}\n",
    "        all_feature = []\n",
    "\n",
    "        min_mag = min(target_mags)\n",
    "        min_patch_size = int(base_patch_size*(base_mag/min_mag))\n",
    "\n",
    "        for cur_mag in target_mags:\n",
    "            if cur_mag!=float(\"inf\"):\n",
    "                cur_patch_size = int(base_patch_size*(base_mag/cur_mag))\n",
    "                cur_feature_path = f\"{feature_path}/mag{cur_mag}x_patch512_{cur_patch_size}/h5_files/{h5_name}\"\n",
    "            else:\n",
    "                cur_feature_path = f\"{feature_path}/thumbnail/h5_files/{h5_name}\"\n",
    "            h5_content = h5py.File(cur_feature_path,'r')\n",
    "            h5_features = h5_content[\"/features\"][:]\n",
    "            h5_coords = h5_content[\"/coords\"][:]\n",
    "            if cur_mag == float(\"inf\"):\n",
    "                cur_mag = 0\n",
    "            feature_i[cur_mag] = {\n",
    "                \"features\": h5_features,\n",
    "                \"coords\": h5_coords\n",
    "            }\n",
    "            if len(h5_features.shape)<2:\n",
    "                h5_features = np.expand_dims(h5_features, 0)\n",
    "            all_feature.append(h5_features)\n",
    "\n",
    "        # generate the Data\n",
    "        # 0. preparation\n",
    "        region_mag = list(feature_i.keys())[1]\n",
    "        patch_mag = list(feature_i.keys())[2]\n",
    "\n",
    "        region_coords = feature_i[region_mag][\"coords\"]\n",
    "        patch_coords = feature_i[patch_mag][\"coords\"]\n",
    "\n",
    "\n",
    "        len_region_node, len_patch_node = len(region_coords), len(patch_coords)\n",
    "\n",
    "        region_patch_size = int((base_mag/region_mag)*base_patch_size)\n",
    "        patch_patch_size = int((base_mag/patch_mag)*base_patch_size)\n",
    "\n",
    "        # 1. x\n",
    "        all_feature = np.concatenate(all_feature, axis=0)\n",
    "\n",
    "        # 2. edge_index_tree_8nb\n",
    "        edge_index = get_edge_index(feature_i, region_patch_size, patch_patch_size)\n",
    "        print(\"edge done\")\n",
    "\n",
    "        # 3. batch\n",
    "        batch = [0]*len(all_feature)\n",
    "\n",
    "        # 4. data_id\n",
    "        data_id = wsi_name.split(\".\")[0]\n",
    "\n",
    "        # 5. node_type\n",
    "        node_type = [0]+[1]*len_region_node+[2]*len_patch_node\n",
    "\n",
    "        # 6. node_tree\n",
    "        node_tree_wo_patch = [-1]+[0]*len_region_node\n",
    "        node_tree_patch = []\n",
    "        for i in range(len_patch_node):\n",
    "            x, y = patch_coords[i]\n",
    "            x_region = int((x//region_patch_size)*region_patch_size)\n",
    "            y_region = int((y//region_patch_size)*region_patch_size)\n",
    "            region_index = np.where((region_coords==(x_region,y_region)).all(axis=1))[0][0]+1\n",
    "            node_tree_patch.append(region_index)\n",
    "        node_tree = node_tree_wo_patch+node_tree_patch\n",
    "\n",
    "        # 7. x_y_index \n",
    "        region_patch_size = int(base_patch_size*(base_mag/target_mags[1]))\n",
    "        patch_patch_size = int(base_patch_size*(base_mag/target_mags[2]))\n",
    "        # print(np.array([[0,0]]).shape, region_coords.shape, patch_coords/patch_patch_size)\n",
    "    \n",
    "        x_y_index = np.concatenate([\n",
    "            np.array([[0,0]]),\n",
    "            region_coords/region_patch_size,\n",
    "            patch_coords/patch_patch_size,\n",
    "        ])\n",
    "    \n",
    "        # generate the Data \n",
    "        node_attr=torch.tensor(all_feature, dtype=torch.float)\n",
    "        edge_index_tree_8nb = torch.tensor(edge_index,dtype=torch.long)\n",
    "        batch = torch.tensor(batch)\n",
    "        node_type = torch.tensor(node_type)\n",
    "        node_tree = torch.tensor(node_tree)\n",
    "        x_y_index = torch.tensor(x_y_index, dtype=torch.float)\n",
    "\n",
    "        data = Data(\n",
    "            x = node_attr,\n",
    "            edge_index_tree_8nb = edge_index_tree_8nb,\n",
    "            data_id = data_id,\n",
    "            batch = batch,\n",
    "            node_type = node_type,\n",
    "            node_tree = node_tree,\n",
    "            x_y_index = x_y_index\n",
    "        )\n",
    "\n",
    "        torch.save(data, f'{save_path}/pt_files/{data_id}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Dataset Split\n",
    "\n",
    "Based on CLAM's `create_split_seq.py`, some changes have been made, the input of the label file has been added, and the task has been set to any number of classificationsï¼Œre-write the `create_split_seq_re.py` file. The `label.csv` needs to include three columns of useful information: case_id, slide_id and label.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--label_path`**: Saving path of the classification label.\n",
    "- **`--seed`**: Random seed.\n",
    "- **`--task`**: Task name.\n",
    "- **`--k`**: Number of splits.\n",
    "- **`--label_frac`**: Fraction of labels.\n",
    "- **`--val_frac`**: Fraction of labels for validation.\n",
    "- **`--test_frac`**: Fraction of labels for test.\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "The command line for dataset split can be structured as follows:\n",
    "\n",
    "```bash\n",
    "python create_splits_seq_re.py --label_path /path/to/dataset_csv/WSI_bm20/label.csv --seed 1 --task WSI_bm20 --label_frac 1.0 --k 10 --val_frac 0.2 --test_frac 0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Training:\n",
    "\n",
    "In `main.py` and `utils/core_utils.py`, some minor additions and changes were made to enable the training of the HIGT model, and re-write the `main_re.py` and `utils/core_utils_re.py` files.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--max_epochs:`**: Maximum number of epochs to train.\n",
    "- **`--label_path:`**: The path of the classification label.\n",
    "- **`--drop_out`**: Enable dropout (p=0.25).\n",
    "- **`--early_stopping`**: Enable early stopping.\n",
    "- **`--lr`**: Learning rate.\n",
    "- **`--k`**:Number of folds.\n",
    "- **`--label_frac`**: Fraction of training labels.\n",
    "- **`--exp_code`**: Experiment code for saving results.\n",
    "- **`--weighted_sample`**: Enable weighted sampling.\n",
    "- **`--bag_loss`**: Slide-level classification loss function.\n",
    "- **`--task`**: Task name.\n",
    "- **`--split_dir`**: Manually specify the set of splits to use.\n",
    "- **`--model_type`**: Model type used for training.\n",
    "- **`--log_data`**: Log data using tensorboard.\n",
    "- **`--data_root_dir`**: Data directory.\n",
    "- **`--results_dir:`**: Desults directory.\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "\n",
    "The command line for GPU training can be structured as follows:\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python main_re.py --max_epochs 100 --label_path /path/to/dataset_csv/WSI_bm20/label.csv --drop_out --early_stopping --lr 2e-4 --k 10 --label_frac 1.0 --exp_code test_binary_100 --weighted_sample --bag_loss ce --task WSI_bm20 --split_dir /path/to/exp/splits/WSI_bm20 --model_type HIGT --log_data --data_root_dir /path/to/exp/extracted_feature/WSI_bm20/graph --results_dir /path/to/exp/result/WSI_bm20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing and Evaluation:\n",
    "\n",
    "\n",
    "In `eval.py`, some minor additions and changes were made to enable the testing and evaluating of the HIGT model, and re-write the `eval_re.py` file.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--drop_out`**: Enable dropout (p=0.25).\n",
    "- **`--k`**: Number of folds.\n",
    "- **`--models_exp_code`**: Fraction of training labels.\n",
    "- **`--save_exp_code`**: Experiment code for saving results.\n",
    "- **`--task`**: Task name.\n",
    "- **`--model_type`**: Model type used for training.\n",
    "- **`--results_dir:`**: Results directory.\n",
    "- **`--split_dir`**: Manually specify the set of splits to use.\n",
    "- **`--data_root_dir`**: Data directory.\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "\n",
    "The command line for testing and evaluation can be structured as follows:\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python eval_re.py --drop_out --k 10 --models_exp_code test_binary_100 --save_exp_code WSI_bm20 --task WSI_bm20 --model_type HIGT --results_dir /path/to/exp/result/WSI_bm20 --split_dir /path/to/exp/splits/WSI_bm20 --data_root_dir /path/to/exp/extracted_feature/WSI_bm20/graph\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
